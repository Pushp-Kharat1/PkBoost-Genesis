{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2Bz9kJrZYF7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Setup and Installations\n",
        "# -------------------------------\n",
        "# This cell installs all required Python libraries for the project.\n",
        "!pip install pybind11 kaggle -q\n",
        "print(\"pybind11 and kaggle insyalled\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTtSK4t0cNcN",
        "outputId": "71ce7601-b064-403c-fc6b-c18512bb565f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/293.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/293.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hpybind11 and kaggle insyalled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pknumpy_shannon_v2.cpp\n",
        "// Cell 2: Fixed C++ Backend (Optimized Shannon).\n",
        "\n",
        "#include <pybind11/pybind11.h>\n",
        "#include <pybind11/stl.h>\n",
        "#include <pybind11/numpy.h>\n",
        "#include <vector>\n",
        "#include <numeric>\n",
        "#include <cmath>\n",
        "#include <algorithm>\n",
        "#include <stdexcept>\n",
        "#include <omp.h> // For Openmp\n",
        "\n",
        "namespace py = pybind11;\n",
        "\n",
        "// Loss Functions\n",
        "py::array_t<double> sigmoid(py::array_t<double> x) {\n",
        "    auto buf = x.request(); py::array_t<double> result(buf.size); auto res_buf = result.request();\n",
        "    double *x_ptr = static_cast<double *>(buf.ptr), *res_ptr = static_cast<double *>(res_buf.ptr);\n",
        "    #pragma omp parallel for\n",
        "    for (ssize_t i=0; i<buf.size; ++i) {\n",
        "        double val = std::max(-500.0, std::min(500.0, x_ptr[i]));\n",
        "        res_ptr[i] = (val >= 0) ? (1.0 / (1.0 + std::exp(-val))) : (std::exp(val) / (1.0 + std::exp(val)));\n",
        "    }\n",
        "    std::vector<ssize_t> shape(x.shape(), x.shape() + x.ndim());\n",
        "    return result.reshape(shape);\n",
        "}\n",
        "\n",
        "py::array_t<double> gradient(py::array_t<double> y_true, py::array_t<double> y_pred) {\n",
        "    auto y_true_buf=y_true.request(); py::array_t<double> p=sigmoid(y_pred); auto p_buf=p.request();\n",
        "    py::array_t<double> result(y_true_buf.size); auto res_buf=result.request();\n",
        "    double *y_true_ptr=static_cast<double*>(y_true_buf.ptr), *p_ptr=static_cast<double*>(p_buf.ptr), *res_ptr=static_cast<double*>(res_buf.ptr);\n",
        "    #pragma omp parallel for\n",
        "    for (ssize_t i=0; i<y_true_buf.size; ++i) res_ptr[i] = p_ptr[i] - y_true_ptr[i];\n",
        "    std::vector<ssize_t> shape(y_true.shape(), y_true.shape() + y_true.ndim());\n",
        "    return result.reshape(shape);\n",
        "}\n",
        "\n",
        "py::array_t<double> hessian(py::array_t<double> y_true, py::array_t<double> y_pred) {\n",
        "    py::array_t<double> p=sigmoid(y_pred); auto p_buf=p.request(); py::array_t<double> result(p_buf.size);\n",
        "    auto res_buf = result.request(); double *p_ptr=static_cast<double*>(p_buf.ptr), *res_ptr=static_cast<double*>(res_buf.ptr);\n",
        "    #pragma omp parallel for\n",
        "    for (ssize_t i=0; i<p_buf.size; ++i) res_ptr[i] = std::max(p_ptr[i] * (1.0 - p_ptr[i]), 1e-7);\n",
        "    std::vector<ssize_t> shape(p.shape(), p.shape() + p.ndim());\n",
        "    return result.reshape(shape);\n",
        "}\n",
        "\n",
        "double init_score(py::array_t<double> y_true) {\n",
        "    auto buf = y_true.request(); if (buf.size == 0) return 0.0;\n",
        "    double sum = std::accumulate(static_cast<double*>(buf.ptr), static_cast<double*>(buf.ptr) + buf.size, 0.0);\n",
        "    double p = std::max(1e-7, std::min(1.0 - 1e-7, sum / buf.size));\n",
        "    return std::log(p / (1.0 - p));\n",
        "}\n",
        "\n",
        "// Helper for Shannon Entropy\n",
        "double calculate_shannon_entropy(double count0, double count1) {\n",
        "    double total = count0 + count1;\n",
        "    if (total < 1e-9 || count0 < 1e-9 || count1 < 1e-9) return 0.0;\n",
        "    double p0 = count0 / total, p1 = count1 / total;\n",
        "    return -p0 * std::log2(p0) - p1 * std::log2(p1);\n",
        "}\n",
        "\n",
        "// Result Structure\n",
        "struct HistSplitResult {\n",
        "    double best_gain = -std::numeric_limits<double>::infinity();\n",
        "    int best_bin_idx = 0;\n",
        "};\n",
        "\n",
        "// Histogram Structure\n",
        "struct Histogram {\n",
        "    std::vector<double> sum_gradients, sum_hessians, sum_y_true, counts;\n",
        "};\n",
        "\n",
        "// Split Finding Function\n",
        "HistSplitResult find_best_split_shannon(\n",
        "    py::array_t<int> x_binned, py::array_t<double> y, py::array_t<double> grad, py::array_t<double> hess,\n",
        "    int n_bins, int min_samples_split, double min_child_weight, double reg_lambda,\n",
        "    double gamma, double mi_weight, int depth)\n",
        "{\n",
        "    auto x_buf=x_binned.request(), y_buf=y.request(), g_buf=grad.request(), h_buf=hess.request();\n",
        "    ssize_t n_samples = x_buf.shape[0];\n",
        "    int *x_ptr=static_cast<int*>(x_buf.ptr); double *y_ptr=static_cast<double*>(y_buf.ptr);\n",
        "    double *g_ptr=static_cast<double*>(g_buf.ptr), *h_ptr=static_cast<double*>(h_buf.ptr);\n",
        "\n",
        "    // Thread-local histograms\n",
        "    int n_threads = omp_get_max_threads();\n",
        "    std::vector<Histogram> private_hists;\n",
        "    for (int i = 0; i < n_threads; ++i) {\n",
        "        private_hists.push_back({std::vector<double>(n_bins,0.0), std::vector<double>(n_bins,0.0),\n",
        "                                std::vector<double>(n_bins,0.0), std::vector<double>(n_bins,0.0)});\n",
        "    }\n",
        "\n",
        "    #pragma omp parallel for\n",
        "    for(ssize_t i=0; i<n_samples; ++i) {\n",
        "        int thread_id = omp_get_thread_num();\n",
        "        int bin = x_ptr[i];\n",
        "        if (bin>=0 && bin<n_bins) {\n",
        "            private_hists[thread_id].sum_gradients[bin]+=g_ptr[i];\n",
        "            private_hists[thread_id].sum_hessians[bin]+=h_ptr[i];\n",
        "            private_hists[thread_id].sum_y_true[bin]+=y_ptr[i];\n",
        "            private_hists[thread_id].counts[bin]+=1.0;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Reduce histograms\n",
        "    Histogram hist{std::vector<double>(n_bins,0.0), std::vector<double>(n_bins,0.0),\n",
        "                   std::vector<double>(n_bins,0.0), std::vector<double>(n_bins,0.0)};\n",
        "    for (int bin = 0; bin < n_bins; ++bin) {\n",
        "        for (int i = 0; i < n_threads; ++i) {\n",
        "            hist.sum_gradients[bin] += private_hists[i].sum_gradients[bin];\n",
        "            hist.sum_hessians[bin] += private_hists[i].sum_hessians[bin];\n",
        "            hist.sum_y_true[bin] += private_hists[i].sum_y_true[bin];\n",
        "            hist.counts[bin] += private_hists[i].counts[bin];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    double G_total=std::accumulate(hist.sum_gradients.begin(),hist.sum_gradients.end(),0.0);\n",
        "    double H_total=std::accumulate(hist.sum_hessians.begin(),hist.sum_hessians.end(),0.0);\n",
        "    double y_total_sum=std::accumulate(hist.sum_y_true.begin(),hist.sum_y_true.end(),0.0);\n",
        "    double n_total_samples=std::accumulate(hist.counts.begin(),hist.counts.end(),0.0);\n",
        "\n",
        "    if (n_total_samples < min_samples_split) return HistSplitResult();\n",
        "\n",
        "    double parent_entropy = calculate_shannon_entropy(n_total_samples - y_total_sum, y_total_sum);\n",
        "\n",
        "    double GL=0.0, HL=0.0, y_left_sum=0.0, n_left_samples=0.0;\n",
        "    HistSplitResult best_split;\n",
        "\n",
        "    for (int i=0; i<n_bins-1; ++i) {\n",
        "        GL+=hist.sum_gradients[i]; HL+=hist.sum_hessians[i];\n",
        "        y_left_sum+=hist.sum_y_true[i]; n_left_samples+=hist.counts[i];\n",
        "\n",
        "        if (HL < min_child_weight || n_left_samples < min_samples_split) continue;\n",
        "        double GR=G_total-GL, HR=H_total-HL;\n",
        "        double n_right_samples = n_total_samples - n_left_samples;\n",
        "        if (HR < min_child_weight || n_right_samples < min_samples_split) continue;\n",
        "\n",
        "        double newton_gain = 0.5*((GL*GL/(HL+reg_lambda))+(GR*GR/(HR+reg_lambda))-(G_total*G_total/(H_total+reg_lambda))) - gamma;\n",
        "\n",
        "        double y_right_sum = y_total_sum - y_left_sum;\n",
        "        double h_left = calculate_shannon_entropy(n_left_samples - y_left_sum, y_left_sum);\n",
        "        double h_right = calculate_shannon_entropy(n_right_samples - y_right_sum, y_right_sum);\n",
        "        double h_conditional = (n_left_samples / n_total_samples) * h_left + (n_right_samples / n_total_samples) * h_right;\n",
        "        double mutual_info = parent_entropy - h_conditional;\n",
        "\n",
        "        double adaptive_weight = mi_weight * std::exp(-0.1 * static_cast<double>(depth));\n",
        "        double combined_gain = newton_gain + adaptive_weight * mutual_info;\n",
        "\n",
        "        if (combined_gain > best_split.best_gain) {\n",
        "            best_split.best_gain = combined_gain;\n",
        "            best_split.best_bin_idx = i;\n",
        "        }\n",
        "    }\n",
        "    return best_split;\n",
        "}\n",
        "\n",
        "// Module Definition\n",
        "PYBIND11_MODULE(pknumpy_shannon_v2, m) {\n",
        "    m.doc() = \"PKNumpy-Shannon\";\n",
        "\n",
        "    // Export functions\n",
        "    m.def(\"sigmoid\", &sigmoid, \"Sigmoid activation function\");\n",
        "    m.def(\"gradient\", &gradient, \"Gradient calculation\");\n",
        "    m.def(\"hessian\", &hessian, \"Hessian calculation\");\n",
        "    m.def(\"init_score\", &init_score, \"Initialize base score\");\n",
        "    m.def(\"find_best_split_shannon\", &find_best_split_shannon, \"Find best split using Shannon MI\");\n",
        "\n",
        "    // Export result class\n",
        "    py::class_<HistSplitResult>(m, \"HistSplitResult\")\n",
        "        .def(py::init<>())\n",
        "        .def_readonly(\"best_gain\", &HistSplitResult::best_gain)\n",
        "        .def_readonly(\"best_bin_idx\", &HistSplitResult::best_bin_idx);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQtK9POYcltF",
        "outputId": "875aafca-f187-4a2a-d335-880c2d93c46a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pknumpy_shannon_v2.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Fixed Manual Compilation and Import\n",
        "import sys\n",
        "import pybind11\n",
        "import importlib.util\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Clear any existing module to avoid registration conflicts\n",
        "if 'pknp_shannon' in globals():\n",
        "    del pknp_shannon\n",
        "\n",
        "# Remove any existing compiled modules\n",
        "for file in os.listdir('/content/'):\n",
        "    if file.startswith('pknumpy_shannon') and (file.endswith('.so') or '.cpython-' in file):\n",
        "        try:\n",
        "            os.remove(f'/content/{file}')\n",
        "            print(f\"Removed existing module: {file}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "pknp_shannon = None\n",
        "py_include = f\"-I/usr/include/python{sys.version_info.major}.{sys.version_info.minor}\"\n",
        "pybind_include = f\"-I{pybind11.get_include()}\"\n",
        "\n",
        "try:\n",
        "    extension_suffix = subprocess.check_output(['python3-config', '--extension-suffix'], text=True).strip()\n",
        "    module_name = f\"pknumpy_shannon_v2{extension_suffix}\"  # Use different name to avoid conflicts\n",
        "\n",
        "    # Compile with proper flags\n",
        "    compile_command = f\"g++ -O3 -shared -fPIC -std=c++17 -fopenmp {py_include} {pybind_include} pknumpy_shannon.cpp -o {module_name}\"\n",
        "\n",
        "    print(\"Compiling Optimized Shannon backend with multi-threading...\")\n",
        "    compile_process = subprocess.run(compile_command, shell=True, check=True, capture_output=True, text=True)\n",
        "\n",
        "    print(\"Compilation successful.\")\n",
        "\n",
        "    # Import the module\n",
        "    module_path = f\"/content/{module_name}\"\n",
        "    spec = importlib.util.spec_from_file_location(\"pknumpy_shannon\", module_path)\n",
        "    pknp_shannon = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(pknp_shannon)\n",
        "\n",
        "    # Verify the module has the required functions\n",
        "    required_functions = ['sigmoid', 'gradient', 'hessian', 'init_score', 'find_best_split_shannon']\n",
        "    missing_functions = [func for func in required_functions if not hasattr(pknp_shannon, func)]\n",
        "\n",
        "    if missing_functions:\n",
        "        print(f\"ERROR: Module missing functions: {missing_functions}\")\n",
        "        print(\"Available attributes:\", [attr for attr in dir(pknp_shannon) if not attr.startswith('_')])\n",
        "        pknp_shannon = None\n",
        "    else:\n",
        "        print(\"Successfully imported Optimized Shannon C++ module as `pknp_shannon`.\")\n",
        "        print(\"All required functions are available.\")\n",
        "\n",
        "except (FileNotFoundError, subprocess.CalledProcessError, ImportError) as e:\n",
        "    print(f\"ERROR compiling/importing backend: {e}\")\n",
        "    if hasattr(e, 'stderr') and e.stderr:\n",
        "        print(\"Compilation stderr:\", e.stderr)\n",
        "    if hasattr(e, 'stdout') and e.stdout:\n",
        "        print(\"Compilation stdout:\", e.stdout)\n",
        "    pknp_shannon = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eySk2FUcpIC",
        "outputId": "28d930fb-da96-4cd5-bee6-1979f24c48fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling Optimized Shannon backend with multi-threading...\n",
            "ERROR compiling/importing backend: Command 'g++ -O3 -shared -fPIC -std=c++17 -fopenmp -I/usr/include/python3.12 -I/usr/local/lib/python3.12/dist-packages/pybind11/include pknumpy_shannon.cpp -o pknumpy_shannon_v2.cpython-310-x86_64-linux-gnu.so' returned non-zero exit status 1.\n",
            "Compilation stderr: cc1plus: fatal error: pknumpy_shannon.cpp: No such file or directory\n",
            "compilation terminated.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: The Final PKBoostShannon Python Class\n",
        "# This cell defines the final, clean Python classes that use the\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SmartHistogramBuilder:\n",
        "    def __init__(self, max_bins: int=32): self.max_bins=max_bins; self.bin_edges_=None; self.n_bins_per_feature_=None\n",
        "    def fit(self, X:np.ndarray):\n",
        "        n_features=X.shape[1]; self.bin_edges_=[]; self.n_bins_per_feature_=[]\n",
        "        for i in range(n_features):\n",
        "            unique_vals=np.unique(X[:, i]);\n",
        "            if len(unique_vals) <= self.max_bins: edges=unique_vals\n",
        "            else: edges=np.unique(np.quantile(X[:, i], np.linspace(0, 1, self.max_bins + 1)))\n",
        "            self.bin_edges_.append(edges); self.n_bins_per_feature_.append(len(edges))\n",
        "        return self\n",
        "    def transform(self, X:np.ndarray) -> np.ndarray:\n",
        "        X_binned=np.zeros_like(X, dtype=np.int32)\n",
        "        for i in range(X.shape[1]):\n",
        "            X_binned[:, i] = np.clip(np.searchsorted(self.bin_edges_[i], X[:, i], side='right') - 1, 0, self.n_bins_per_feature_[i] - 2)\n",
        "        return X_binned\n",
        "\n",
        "class CppShannonLoss:\n",
        "    @staticmethod\n",
        "    def sigmoid(x): return pknp_shannon.sigmoid(x)\n",
        "    def gradient(self, y, p): return pknp_shannon.gradient(y, p)\n",
        "    def hessian(self, y, p): return pknp_shannon.hessian(y, p)\n",
        "    def init_score(self, y): return pknp_shannon.init_score(y)\n",
        "\n",
        "class SimpleTreeShannon:\n",
        "    def __init__(self, max_depth:int=6): self.max_depth=max_depth; self.nodes={}\n",
        "    def fit(self, X_binned, y, grad, hess, params):\n",
        "        self._build_tree(X_binned, y, grad, hess, 0, 0, params)\n",
        "    def _build_tree(self, X_binned, y, grad, hess, node_id, depth, params):\n",
        "        n_samples, G, H = X_binned.shape[0], np.sum(grad), np.sum(hess)\n",
        "        if depth>=self.max_depth or n_samples<params['min_samples_split'] or H<params['min_child_weight']:\n",
        "            self.nodes[node_id]={'is_leaf':True, 'value': -G / (H + params['reg_lambda'])}; return\n",
        "        best_gain, best_feature, best_threshold_bin = -np.inf, -1, 0\n",
        "        for feat_idx in range(X_binned.shape[1]):\n",
        "            result = pknp_shannon.find_best_split_shannon(\n",
        "                np.ascontiguousarray(X_binned[:, feat_idx], dtype=np.int32), np.ascontiguousarray(y),\n",
        "                np.ascontiguousarray(grad), np.ascontiguousarray(hess),\n",
        "                params['n_bins_per_feature'][feat_idx], params['min_samples_split'], params['min_child_weight'],\n",
        "                params['reg_lambda'], params['gamma'], params['mi_weight'], depth\n",
        "            )\n",
        "            gain, threshold_bin = result.best_gain, result.best_bin_idx\n",
        "            if gain > best_gain: best_gain, best_feature, best_threshold_bin = gain, feat_idx, threshold_bin\n",
        "        if best_gain <= 1e-6:\n",
        "            self.nodes[node_id]={'is_leaf':True, 'value':-G/(H+params['reg_lambda'])}; return\n",
        "        self.nodes[node_id]={'is_leaf':False,'feature':best_feature,'threshold':best_threshold_bin,'left_child':2*node_id+1,'right_child':2*node_id+2}\n",
        "        left_mask = X_binned[:, best_feature] <= best_threshold_bin\n",
        "        self._build_tree(X_binned[left_mask], y[left_mask], grad[left_mask], hess[left_mask], 2*node_id+1, depth+1, params)\n",
        "        self._build_tree(X_binned[~left_mask], y[~left_mask], grad[~left_mask], hess[~left_mask], 2*node_id+2, depth+1, params)\n",
        "    def predict(self, X_binned:np.ndarray)->np.ndarray:\n",
        "        preds = np.zeros(X_binned.shape[0])\n",
        "        for i in range(X_binned.shape[0]):\n",
        "            node_id=0\n",
        "            while node_id in self.nodes:\n",
        "                node=self.nodes[node_id]\n",
        "                if node['is_leaf']: preds[i]=node['value']; break\n",
        "                node_id = node['left_child'] if X_binned[i, node['feature']] <= node['threshold'] else node['right_child']\n",
        "        return preds\n",
        "\n",
        "class PKBoostShannon:\n",
        "    def __init__(self, n_estimators=500, learning_rate=0.1, max_depth=6, min_samples_split=20,\n",
        "                 min_child_weight=1.0, reg_lambda=1.0, gamma=0.0, subsample=0.8,\n",
        "                 colsample_bytree=0.8, early_stopping_rounds=50, histogram_bins=32,\n",
        "                 random_state=None, mi_weight=0.1):\n",
        "        params=locals(); del params[\"self\"];\n",
        "        for k, v in params.items(): setattr(self, k, v)\n",
        "        self.trees:List[SimpleTreeShannon]=[]; self.base_score,self.best_iteration,self.best_score=0.0,0,-np.inf\n",
        "        self.fitted=False; self.loss_fn=CppShannonLoss()\n",
        "        if random_state is not None: np.random.seed(random_state)\n",
        "    def fit(self, X, y, eval_set=None, verbose=True):\n",
        "        start_time=time.time(); X, y=np.asarray(X), np.asarray(y)\n",
        "        n_samples, n_features = X.shape; self.base_score=self.loss_fn.init_score(y)\n",
        "        train_preds=np.full(n_samples, self.base_score)\n",
        "        self.histogram_builder = SmartHistogramBuilder(self.histogram_bins).fit(X)\n",
        "        X_processed = self.histogram_builder.transform(X)\n",
        "        if eval_set:\n",
        "            X_val, y_val=np.asarray(eval_set[0]), np.asarray(eval_set[1])\n",
        "            X_val_processed = self.histogram_builder.transform(X_val)\n",
        "            val_preds = np.full(len(y_val), self.base_score)\n",
        "        if verbose: print(f\"PKBoost Training Started on {n_samples} samples\")\n",
        "        for iteration in range(self.n_estimators):\n",
        "            s_idxs=np.random.choice(n_samples, int(self.subsample*n_samples), replace=False)\n",
        "            f_idxs=np.random.choice(n_features, max(1, int(self.colsample_bytree*n_features)), replace=False)\n",
        "            grad, hess = self.loss_fn.gradient(y[s_idxs], train_preds[s_idxs]), self.loss_fn.hessian(y[s_idxs], train_preds[s_idxs])\n",
        "\n",
        "            tree = SimpleTreeShannon(self.max_depth)\n",
        "            tree_params = {p: getattr(self,p) for p in [\"min_samples_split\", \"min_child_weight\", \"reg_lambda\", \"gamma\", \"mi_weight\"]}\n",
        "            tree_params['n_bins_per_feature'] = [self.histogram_builder.n_bins_per_feature_[i] for i in f_idxs]\n",
        "\n",
        "            tree.fit(X_processed[s_idxs][:, f_idxs], y[s_idxs], grad, hess, tree_params)\n",
        "            tree.feature_indices=f_idxs; self.trees.append(tree)\n",
        "            train_preds += self.learning_rate * tree.predict(X_processed[:, f_idxs])\n",
        "            if eval_set:\n",
        "                val_preds += self.learning_rate * tree.predict(X_val_processed[:, f_idxs])\n",
        "                val_roc = roc_auc_score(y_val, self.loss_fn.sigmoid(val_preds))\n",
        "                if val_roc > self.best_score: self.best_score, self.best_iteration=val_roc, iteration\n",
        "                elif iteration-self.best_iteration>=self.early_stopping_rounds:\n",
        "                    if verbose: print(f\"Early stopping at iter {iteration+1}\"); break\n",
        "            else: self.best_iteration = iteration\n",
        "            if verbose and (iteration+1)%50==0 and eval_set: print(f\"[{iteration+1:3d}] Val ROC: {val_roc:.4f}\")\n",
        "        self.fitted=True\n",
        "        if verbose: print(f\"Training completed in {time.time()-start_time:.2f}s. Best Val ROC: {self.best_score:.6f}\")\n",
        "        return self\n",
        "    def predict_proba(self, X:np.ndarray)->np.ndarray:\n",
        "        if not self.fitted: raise ValueError(\"Model not fitted\")\n",
        "        X_proc=self.histogram_builder.transform(np.asarray(X))\n",
        "        preds=np.full(X_proc.shape[0], self.base_score)\n",
        "        for tree in self.trees[:self.best_iteration+1]:\n",
        "            preds += self.learning_rate * tree.predict(X_proc[:, tree.feature_indices])\n",
        "        return self.loss_fn.sigmoid(preds)\n",
        "print(\"Python classes for PkBoost defined successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNG4aNKyczdw",
        "outputId": "c39c1dee-d7a3-47f1-9600-d156952fd4b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python classes for PkBoost defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Cell: Head-to-Head Benchmark vs. LightGBM\n",
        "# -----------------------------------------------\n",
        "# This cell installs lightgbm and then runs a direct comparison\n",
        "# between your optimized PKBoostShannon model and LightGBM on the\n",
        "# prepared Credit Card Fraud dataset.\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import time\n",
        "\n",
        "# --- Ensure lightgbm is installed ---\n",
        "!pip install lightgbm -q\n",
        "\n",
        "# Check if data is loaded and the C++ module is ready\n",
        "if 'data_loaded_successfully' in locals() and data_loaded_successfully and 'pknp_shannon' in locals():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL BENCHMARK: PKBoost vs. LightGBM\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # --- 1. Your PKBoost (Optimized Shannon) Model ---\n",
        "    print(\"\\n1. PkBoost MODEL...\")\n",
        "    pkb_model = PKBoostShannon(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=5,\n",
        "        early_stopping_rounds=30,\n",
        "        random_state=42,\n",
        "        mi_weight=0.5\n",
        "    )\n",
        "    pkb_start_time = time.time()\n",
        "    pkb_model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
        "    pkb_time = time.time() - pkb_start_time\n",
        "    pkb_roc = roc_auc_score(y_test, pkb_model.predict_proba(X_test))\n",
        "    print(f\"PKBoost training complete.\")\n",
        "\n",
        "    # --- 2. LightGBM Model ---\n",
        "    print(\"\\n2. TRAINING LIGHTGBM MODEL...\")\n",
        "    lgbm_model = lgb.LGBMClassifier(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1 # Use all available cores\n",
        "    )\n",
        "    lgbm_start_time = time.time()\n",
        "    # Use a callback for early stopping in LightGBM\n",
        "    lgbm_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric='auc',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=30, verbose=False)]\n",
        "    )\n",
        "    lgbm_time = time.time() - lgbm_start_time\n",
        "    lgbm_roc = roc_auc_score(y_test, lgbm_model.predict_proba(X_test)[:, 1])\n",
        "    print(f\"LightGBM training complete.\")\n",
        "\n",
        "    # --- 3. Logistic Regression Baseline ---\n",
        "    lr = LogisticRegression(max_iter=1000, solver='lbfgs', random_state=42, class_weight='balanced')\n",
        "    lr.fit(X_train,y_train)\n",
        "    lr_roc = roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1])\n",
        "\n",
        "    # --- Final Comparison ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL PERFORMANCE COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Model':<40} {'Test ROC-AUC':<15} {'Time (s)':<10}\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Logistic Regression':<40} {lr_roc:<15.6f} {'N/A'}\")\n",
        "    print(f\"{'PKBoost ':<40} {pkb_roc:<15.6f} {pkb_time:<10.2f}\")\n",
        "    print(f\"{'LightGBM ':<40} {lgbm_roc:<15.6f} {lgbm_time:<10.2f}\")\n",
        "    print(\"=\"*70)\n",
        "else:\n",
        "    print(\"\\nSkipping final benchmark\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBZAHLh-dVgC",
        "outputId": "2dc6ca99-2979-4349-f1ee-8500dd35208a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL BENCHMARK: PKBoost vs. LightGBM\n",
            "================================================================================\n",
            "\n",
            "1. TRAINING YOUR PKBOOST (OPTIMIZED SHANNON) MODEL...\n",
            "  -> PKBoost training complete.\n",
            "\n",
            "2. TRAINING LIGHTGBM MODEL...\n",
            "[LightGBM] [Info] Number of positive: 275, number of negative: 28000\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007624 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 28275, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.009726 -> initscore=-4.623189\n",
            "[LightGBM] [Info] Start training from score -4.623189\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "  -> LightGBM training complete.\n",
            "\n",
            "================================================================================\n",
            "FINAL PERFORMANCE COMPARISON\n",
            "================================================================================\n",
            "Model                                    Test ROC-AUC    Time (s)  \n",
            "----------------------------------------------------------------------\n",
            "Logistic Regression                      0.967041        N/A\n",
            "PKBoost (Your Optimized Shannon Formula) 0.977481        48.72     \n",
            "LightGBM (Industry Standard)             0.967728        0.60      \n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}